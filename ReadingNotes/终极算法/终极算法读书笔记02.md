#终极算法读书笔记02

date: 2018-06-22 16:04:01
tags: 终极算法


### 第三章 符号学派：休谟的归纳问题

在本章中作者从理论主义者和经验主义者出发，介绍了理论主义者和经验主义者的区别，其中列举了很多例子，比如：
- 律师、数学家、柏拉图、笛卡尔、莱布尼茨是理论主义者
- 医生、科学家、亚里士多德、休谟、洛克是经验主义者

引出似乎机器学习是经验主义的产物

之后通过约会的例子，根据历史的经验数据归纳，很难推广到没有见过的事情中；而且在数量量太大的时候，需要决定的时间存在于数据库，但是概率太小，小道可以忽略；其中通过垃圾邮件分类的例子，每个垃圾邮件不完全一样，所以对于预测问题的能力需要一般化（泛化）

`天下没有免费的午餐`的定理：没有那个学习算法可以比得上随意猜测。但是当我们对世界有了解，把指数输入到学习算法中，那么相比于随意猜测就会有优势。没有知识，就无法学习

在进行归纳的时候，通常会对问题的边界进行限定，比如：两个因素的组合预测失败，可以尝试多个因素的组合，这是`合取概念`，比如字典中对词语的定义就是如此：
- 椅子是有靠背，若干条腿的坐具，其中任何一个描述去掉就不在是椅子

在进行机器学习的时候，会做有条件的假设，如果如法解释数据，就放宽条件，在这个过程中，作者描述了在合取概念太多时候，如果进行对问题进行归纳

之后，作者指出，机器学习是处于无知和幻觉中间的，机器学习需要知识，但是还需要遗忘细节，记住重要的部分；没有知识，会出现欠拟合，记住了全部的细节，会出现过拟合

当假设过多，但是数据不够区分这些假设的时候，会出现过拟合，当算法要做的事情和输入的数据一样呈现出指数级增长的时候，出现组合爆炸

学习就是拥有的数据的数量和假设数量之间的较量

之后提出如何信任学习算法，可以对训练数据进行划分，分为训练集合测试集，通过测试集观察算法的可靠性，但是及时测试集准确度也会出现问题，比如当输入的数据不合适的时候

通过运用统计显著性检验也可以确定模型可靠：在构建规则的时候，如果某一时刻无法找到提高该规则的准确度的条件，那么应该停止，及时还包含负面的例子，这样会降低规则训练集的准确度，但是能够保障一定的泛化能力

奥卡姆剃刀原理：如无必要，勿增实体；切勿浪费较多东西去做用较少的东西同样可以做好的事情

之后作者解释偏差和方差：
- 偏差：距离预测结果的远近。越大越远
- 方差：预测结果的稳定程度。越大越分散

之后作者提出归纳是逆向的演绎，通过结果理解其过程，但是逆向演绎的局限在于，设计到密集计算的时候，无法扩展到海量的运算中，为此符号学家选择了决策树进行归纳。在决策树中无法使用准确率对决策树的节点进行划分（不是预测类别，而是对类别精心划分，让其变得纯粹），所以使用熵：用来衡量混乱度的单位。如果出现过拟合，依然可以使用显著性检验或者是限制树的大小
- 2002年，决策树准确的预测了3/4的最高法院裁决，但是专家小组准确率不大60%

20世纪70年代，基于知识的系统取得了卓越成绩，但是到了80年代消失了。从专家身上提取知识，编码规则难度太大，但是让程序从患者症状和疗效的数据库只能够自己学习，更容易。

逆向演绎的结果直观，容易解释和接收，但是并不是所有的概念都能够用规则集来定义，还有跟多东西无法通过形式的推理看到，如果我们的潜意识。这也是联结学派对符号学派不满的地方

### 第四章 联结学派：大脑如何学习
在本章中作者从神经元出发，介绍了联结学派和符号学派的区别：
- 符号和代表的概念之间有一一对应的关系，但是；联结学派的代表方式却是分散的
- 符号学派是按次序的，联结学派是平行的

之后作者极少了弗兰克.罗森布拉特的感知机原理：接收不同输入量，乘以不同的权重相加和，经过阶越函数(激活函数)得到结果；此时激活函数是一个线性函数，所以只能处理线性问题，无法解决异或问题，导致神经网络逐渐淡出人们的视线

直到1985年，大卫.艾可利，辛顿等人把霍普菲尔德网络里面的确定性神经元使用可能性神经元代替，神经元的状态就有了概率分布。他们把这种特定的神经元组成的网络成为玻尔兹曼机器，但是玻尔兹曼机器的收敛速度很慢

之后作者介绍了sigmiod函数，并且使用该函数代替原始的激活函数，能够解决非线性的问题

之后就是反向传播算法，但是权重会随着特征的数量增多呈现指数级上涨，会发生无法找到全局最优解的问题。同时使用多个神经元能够解决异或问题,作者举了一些列子：
- 多层感知机实现文本发音
- 预测股票市场
- 自动驾驶

在反向传播刚被发现时，当神经网络的层数越来越多，误差信号会越来越散漫;但是现在由于有足够的数据和足够快的计算机速度，反向传播算法被越来越多的应用

之后作者介绍了：自动编码器，20世纪80年代发明的一种多层感知机，输出量和输入量一样，作用是让隐藏层比输入和输出层小衡多，网络会迫使将输入信息编码在更好地比特李，让影藏层能够表现出来，然后网络会解压信息，还原到原来的大小。但是当时的自动编码器学习速度很慢。10余年后，解决办法：
- 让影藏层比输入和输出层大一些
- 把隐藏层的几个单位赶走，能够阻止影藏层对信息的复制，同时把学习变得简单

以上这个解决版发本称之为：稀疏自动编码器

深度学习算法：
- 叠加自动编码器：第一个自动编码器对局部特征进行编码，第二个对更大范围的特征进行编码，第三个自动编码器对再大一些的特征进行编码
- 玻尔兹曼机作为机器的结构
- 把视皮层质作为基础的卷积神经网络

但是符号学派认为多层感知器无法把多个相关的规则穿起来，一旦神经网络学习完成，只会计算同一个指定的函数，整体上神经网络的模型没有决策树和规则集之类的方法简单直观课件

但是神经网络的这些缺点，在人脑中却都不曾出现，那么人脑中的这些能力从何而来:进化，下一章就会开始介绍进化学派的自然学习算法

-----
作者尝试把这本书写得人人都能看懂，但是其中涉及到专业的解释和描述的地址给人的感觉是很模糊的，通过很多的例子，对于本质阐述的不是特别清楚
